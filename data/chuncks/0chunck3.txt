architecture Deep Learning based sequential models have been widely adopted to detect intrusions and anomalies in various type of computer networks 18 19 20 Our proposal in this section is to employ Recurrent Neural Networks or RNN as a sequential model to the labeled dataset generated in the previous section The proposed RNN is presented in Figure 5 Furthermore its resulting hyperparameters are shown in Table V The input to the RNN consists of 60 ordered packets with 195 features each It passes to two stacked RNN layers which have recurrent connections between hidden units The two RNN layers read the entire input sequence of 60 packets and feed their output to a dense layer which produces 5 outputs corresponding to each the 5 classes using softmax function We denote the training set fxtytgT t1 wherext is the feature vector a vector of dimension 195 for sample t2f1Tgytrepresents the corresponding label for sample t2f1Tg andTthe number of samples in the training set Moreover each label in the training set is such that ytis a binary vector of dimension 5 ieyt2B5 where element i2f15gof the vector ytis a binary variable representing whether the corresponding feature vector xt belongs ie corresp entry 1 or does not belong ie corres entry 0 to classi2f15g Furthermore ytmay only have one nonzero entry which follows from our previous assumption that only one intrusion is possible in each sample The equations describing the operation for the RNN are the following atWht1Uxtb8t2f1T1g 1 ht at 2 otVhtc 3 yt ot 4where the vector atis a linear combination between xt the feature vector for sample t and the hidden layer output of the RNN for sample t1ht1htis a vector modeling the hidden layer output of the RNN for sample tota vector of dimension 5 is a linear combination of the output hidden layerhtytis the prediction that RNN outputs for sample xt and has the same properties as ytWUV are the shared weights matrices that will optimized in training and are nonlinear activation functions applied elementbyelement on their respective inputs TABLE V HYPERPARAMETERS Hyperparameters Values Number of layers 3 Number of Neurons per layer 50105 Activation Function per layer tanhtanhsoftmax Optimizer Adam Loss Categorical Cross Entropy Learning Rate 0001 Batch size 100 Epoch size 50 VI E VALUATION METRICS We use the Area Under The Curve AUC values Receiver Operating Characteristics ROC curves and F1 scores and calculate them for each class to assess our IDS performance We also present the multiclass confusion matrices which contains information about the actual and prediction classica tions done by the classier to describe the performance of the multiclassier models The training samples corresponding to the label ground truth yt are represented by each row of the matrix whereas the occurrences in a predicted label yt RNN output are represented by each column Specically for the task at hand the confusion matrix will be a 55 where element ij2 f15gf 15gdenotes the normalized number of occurrences where the true label is from classi2f15g and the predicted label is from class j2f15g Thus for an ideal multiclass classier all the diagonal entries should be 1 while the offdiagonal entries should be 0 In addition to the confusion matrix we use the following other metrics Recall is the ratio of correctly predicted positive observa tions of all the observations in the actual class Recall TP TPFN5 Precision is the ratio of correctly predicted positive observa tions of all the observations in the predicted class Precision TP TPFP6 Hence the F1Score is calculated using the following equa tion F1Score 2PrecisionRecall Precision Recall7Where TP True Positive FP False Positive TN True Negative FN False Negative Since the dataset is imbalanced the F1 score which is the weighted average results of both metrics precision and recall is essential for evaluating the deep learning based IDS performance The model has a large predictive power if the F1 score is near to 10 The receiver operating characteristic curve or ROC curve is a graphical representation of a classier systems performance while its discrimination threshold is modied It is calculated by displaying the true positive rate TPR vs the false positive rate FPR at different threshold levels The ideal classier should provide a point in the upper left corner of the ROC space or coordinate 01 signifying 100 percent sensitivity no false negatives and 100 percent specicity no false positives AUC stands for Area under the ROC Curve It measures the entire twodimensional area underneath the entire ROC curve The AUC for an ideal classier should be 1 In the experiments we use the Python library Keras 2 to implement our RNN model We train and evaluate our model on an IntelR CoreTM i56440HQ CPU 260GHz VII R ESULTS Using the generated dataset we ran a threefold cross validation with early stopping to ensure a large statistical condence for our models prediction performance In each crossvalidation 67 and 33 of the data are chosen at random as the training and validation sets respectively The training set is used for model tting and the validation set is used for model evaluation for each of the hyperparameter sets Furthermore they have the same proportion of classes in each validation fold After crossvalidation we got three trained RNN models To assess the overall performance of our approach we ran three experiments on the testing dataset one for each trained model TABLE VI RESULTS ON VALIDATION DATA Fold Class Recall Precision F1Score 1Normal 099 099 099 Error on Event 1 087 093 Error on Error 061 061 061 Missing Response 093 093 093 Missing Request 093 1 096 2Normal 099 099 099 Error on Event 1 095 097 Error on Error 077 091 083 Missing Response 090 093 091 Missing Request 093 096 095 3Normal 099 099 099 Error on Event 09 1 095 Error on Error 1 087 093 Missing Response 097 088 093 Missing Request 077 087 082 The classication results for threefold crossvalidation are shown in Table VI The experimental results demonstrated that the model performed well with acceptable F1score values for each class of the validation folds Thus the models can classifyFig 6 Confusion matrices